{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "sys.path.append(\"./libs/ACDA\")\n",
    "import Conv_DCFD as acda\n",
    "from crowd import Crowd_sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowd counting\n",
    "The paper uses ACDA trained with ImageNet as feature extractor and replace final linear layer with few standard transposed convolutional layers for recovering \n",
    "\n",
    "Note: wanted to use ShanghaiTech subset A dataset since it is smaller than UCF-QNRF, but couldn't find it. UCF-QNRF dataset available here:\n",
    "https://www.crcv.ucf.edu/data/ucf-qnrf/\n",
    "\n",
    "Using the ShanghaiTech subset A dataset since it is smaller than UCF-QNRF\n",
    "\n",
    "Download link:\n",
    "https://www.kaggle.com/datasets/tthien/shanghaitech\n",
    "\n",
    "Paper that provided download link and the processing of the ShanghaiTech dataset (need to add citation to paper!)\n",
    "https://github.com/cvlab-stonybrook/DM-Count\n",
    "@inproceedings{wang2020DMCount,\n",
    "  title={Distribution Matching for Crowd Counting},\n",
    "  author={Boyu Wang and Huidong Liu and Dimitris Samaras and Minh Hoai},\n",
    "  booktitle={Advances in Neural Information Processing Systems},\n",
    "  year={2020},\n",
    "}\n",
    "\n",
    "For the crowd counting need to implement Resnet-18 (Ad-ResNet-s listed in Table A of appendix) but with out the first \"max pool\" layer and final \"average pool\" layer.\n",
    "The Ad-ResNet-s network was very similar to ResNet-18, so the code for a ResNet-18 was used as a starting point and updated with the changes described in Appendix A.4 for crodwd counting.\n",
    "Reference for how to implement ResNet-18:\n",
    "https://www.kaggle.com/code/ivankunyankin/resnet18-from-scratch-using-pytorch/notebook\n",
    "\n",
    "ResNet original paper: https://arxiv.org/pdf/1512.03385.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base code for ResNet18 came from link below, modified based on Appendix A structure\n",
    "# https://www.kaggle.com/code/ivankunyankin/resnet18-from-scratch-using-pytorch?scriptVersionId=54460301&cellId=23\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, kernel_size_acda = None, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 3\n",
    "        if kernel_size_acda:\n",
    "            self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "            self.acda_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size_acda, stride=1, padding=1)\n",
    "            self.acda_bn = nn.BatchNorm2d(in_channels)\n",
    "            self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet_18(nn.Module):\n",
    "    def __init__(self, image_channels, num_classes):\n",
    "        super(ResNet_18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def __make_layer(self, in_channels, out_channels, stride):\n",
    "        \n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.shape[0], -1)\n",
    "        # x = self.fc(x)\n",
    "        return x \n",
    "\n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "class ResNet_18_ACDA(nn.Module):\n",
    "    def __init__(self, image_channels, num_classes):\n",
    "        super(ResNet_18_ACDA, self).__init__()    \n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        # # first max pooling layer removed\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2, kernel_size=1, kernel_size_acda=7)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2, kernel_size=1, kernel_size_acda=5)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def __make_layer(self, in_channels, out_channels, stride, kernel_size=None, kernel_size_acda=None):\n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "\n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, \n",
    "                  stride=stride, kernel_size=kernel_size, kernel_size_acda=kernel_size_acda), \n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # YVETTE - last maxpool layer also removed\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.shape[0], -1)\n",
    "        # x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "resNet = ResNet_18(3, 1000)\n",
    "resNet_acda = ResNet_18_ACDA(3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "mae = nn.L1Loss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def train(model, optimizer, train_data, num_epochs=2):\n",
    "    loss_vals = []\n",
    "    yvette = 0\n",
    "    for _ in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        # for i, data in enumerate(train_data, 0):\n",
    "        for i, (inputs, points, gt_discrete) in enumerate(train_data):\n",
    "            # get the inputs\n",
    "            gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "            N = inputs.size(0)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            # print('outputs shape: ', outputs.shape, ' gd_count shape: ', torch.from_numpy(gd_count).float().shape)\n",
    "            # mae_loss = mae(outputs, torch.from_numpy(gd_count).float())\n",
    "            mse_loss = mae(outputs.sum(1).sum(1).sum(1), torch.from_numpy(gd_count).float())\n",
    "            mse_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "            pred_err = pred_count - gd_count\n",
    "            if yvette < 5:\n",
    "                print('running_loss = ', running_loss, ' mae_loss.item() = ', mse_loss.item(), ' pred_err = ', pred_err)\n",
    "            yvette += 1\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += mse_loss.item()\n",
    "            if i % 10 == 9:    # print every 2000 mini-batches\n",
    "                # print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                loss_vals.append(running_loss / 10)\n",
    "                running_loss = 0.0\n",
    "    return loss_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resNet_acda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of img: 300\n",
      "number of img: 182\n"
     ]
    }
   ],
   "source": [
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    gt_discretes = torch.stack(transposed_batch[2], 0)\n",
    "    return images, points, gt_discretes\n",
    "\n",
    "# load the data\n",
    "train_path = './data/ShanghaiTech/part_A/train_data/'\n",
    "test_path = './data/ShanghaiTech/part_A/test_data/'\n",
    "\n",
    "train_crowd = Crowd_sh(train_path, crop_size=112, method='train') # YVETTE - should that be 512 instead...?\n",
    "test_crowd = Crowd_sh(test_path, crop_size=112, method='val')\n",
    "\n",
    "# batch_size = 10\n",
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(train_crowd, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0,\n",
    "                                          collate_fn=train_collate,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_crowd, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0,\n",
    "                                         collate_fn=default_collate,\n",
    "                                         pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_loss =  0.0  mae_loss.item() =  14.354814529418945  pred_err =  [-1.1637337e+01  2.6089025e+01  1.3266768e+00 -1.3633568e+01\n",
      " -1.2634254e+01  7.3661537e+00  2.2825956e+00 -2.2561989e+00\n",
      " -2.2633432e+01  4.3621283e+00  5.4725137e+00 -7.6386337e+00\n",
      " -2.2636459e+01  5.3639879e+00  6.6622972e+00 -2.0821482e+01\n",
      " -6.5635056e+01 -3.0047808e+01 -5.9636322e+01  3.2021618e-01\n",
      "  6.3629370e+00 -8.0623055e-01 -1.6795940e+00  1.3586330e+00\n",
      " -1.7634979e+01 -5.6325226e+00  4.4291472e+00 -9.7287397e+00\n",
      " -3.2639744e+01  4.3655329e+00 -9.2633194e+01  3.4177780e-02\n",
      " -1.6452866e+00 -2.4693108e+01  5.2611055e+00 -7.3675537e-01\n",
      " -1.3632218e+01 -3.0260720e+00 -5.5636635e+01 -2.3639269e+01\n",
      "  2.3626394e+00  7.3648024e+00  3.3659172e+00  1.1677519e+01\n",
      "  8.3298016e+00 -2.7633291e+01 -2.7634512e+01  7.3671455e+00\n",
      " -7.3990791e+01  8.3621626e+00  3.6419487e-01  4.6120896e+00\n",
      " -2.4449997e+00 -6.3956070e-01  7.3643398e+00 -4.3575764e+00\n",
      " -1.7632952e+01  7.3598986e+00  3.0514641e+00 -3.5633781e+01\n",
      "  2.7885094e+00  4.3426771e+00 -8.6334829e+00  7.3619461e+00]\n",
      "running_loss =  14.354814529418945  mae_loss.item() =  16.048076629638672  pred_err =  [ 1.41219273e+01 -6.68857193e+01  1.85903430e+00 -9.87611008e+00\n",
      "  9.12601757e+00  1.11267376e+01  1.01254196e+01 -9.87369347e+00\n",
      "  1.75858154e+01  1.12603188e+00 -2.48735352e+01  1.41259766e+01\n",
      "  1.26273155e-01  2.12896729e+00 -2.18732491e+01  1.13000965e+00\n",
      "  1.11277494e+01 -6.87356758e+00  1.24201775e-01 -4.68760223e+01\n",
      "  1.02242928e+01  4.07388783e+00 -7.33365154e+00  1.20417566e+01\n",
      " -6.87128735e+00  2.33630371e+01 -2.08735352e+01  4.10310745e+00\n",
      "  4.38058186e+00  4.64598312e+01 -1.38756742e+01  1.11251373e+01\n",
      " -2.28742828e+01  1.41299019e+01 -1.18745651e+01  7.12624168e+00\n",
      "  1.31268253e+01  1.41230974e+01  1.31274090e+01  1.13931522e+01\n",
      " -4.68787079e+01 -7.07051182e+00  1.41279583e+01  1.41220102e+01\n",
      "  1.41222715e+01  2.12516117e+00  1.20773726e+01  7.12515163e+00\n",
      "  1.15731010e+01  1.12248087e+00 -4.48776588e+01  2.12634087e+00\n",
      "  3.63778305e+00  6.68134022e+00  1.11265602e+01 -4.87211132e+00\n",
      " -4.38727951e+01  1.14709768e+01 -3.48707771e+01  1.01220131e+01\n",
      " -1.33875931e+02 -1.78960743e+01 -5.95751667e+00 -1.18769169e+01]\n",
      "running_loss =  30.402891159057617  mae_loss.item() =  9.841395378112793  pred_err =  [-10.858826     5.141119    -7.7260866   -0.14286184   5.1467395\n",
      "  -0.7104597   -4.860083   -41.857258   -10.872478     6.1260433\n",
      " -15.860428     0.06540585   5.136624    -2.8589296  -41.86193\n",
      "   5.136727     0.9104674    5.1437016   -6.8582125  -50.856316\n",
      " -20.863178   -24.85679    -24.86678     -9.881714   -10.87127\n",
      " -58.85418      2.9825542    4.8482413    1.1400852    5.141915\n",
      "   3.7207875  -39.85675      4.1287017   -8.86201     -6.8585944\n",
      "   5.1428757   -1.8557992    2.143149   -41.85623      3.916461\n",
      "   1.0184528    3.1422825   -1.8590627   -1.8588648    3.1406732\n",
      " -10.85956     -6.8568287    5.1429973   -1.8589396    0.14167643\n",
      "   0.14271164 -13.724142     3.145225    -4.868818    -1.8065448\n",
      "   5.1457753   -4.937536   -15.043217    -3.8571796   25.925661\n",
      "  -1.8599677    1.1410413    1.0627127   -0.5307183 ]\n",
      "running_loss =  40.24428653717041  mae_loss.item() =  17.721834182739258  pred_err =  [   8.534439     -5.458516      2.0314388    18.08292     -66.46115\n",
      "    4.996567      8.4007        8.525169    -31.463669     -1.4610615\n",
      "   12.534185     11.539428      4.542259     11.066291      5.5248566\n",
      "    1.5378084    -8.462121     12.534614     37.017242     12.251236\n",
      "   -7.46257      10.519546     12.539345     12.538398      1.5090666\n",
      "  -54.459427      9.055737     11.540333    -46.058674    -96.46039\n",
      "   -5.461443     -3.4608517    -8.521571      5.1754465    11.536788\n",
      "    4.584013      8.539818      3.6298962  -162.46004      25.724361\n",
      "  -21.458992     12.531798     -8.752451     12.539149     -6.463022\n",
      "    6.541155     30.012337     -2.4617968   -17.463535     12.52169\n",
      "    9.5201435   -18.463272     11.5415        4.5037746     4.8477297\n",
      "   10.470924     -0.91523576   12.535435      2.5385199   -14.46159\n",
      "  -43.466923      8.525895    -83.45737       4.539857  ]\n",
      "running_loss =  57.96612071990967  mae_loss.item() =  11.558281898498535  pred_err =  [  9.07658    -46.926857    -4.425064   -67.92527      7.0733433\n",
      "   3.0740185    8.058549     5.077092    12.279968     7.1902432\n",
      " -49.926373     1.0758352    6.6083136   -3.9299335    4.0743237\n",
      " -22.926643     5.7752705    4.983205    -3.925744   -27.929691\n",
      "   8.076494     6.0767174    6.0732765   -8.950808     2.0736284\n",
      "   9.07095    -43.926758    -5.9349957    3.069788    -2.9237347\n",
      "   0.17370808 -31.924517     2.0741892   -4.924466   -12.92488\n",
      "   0.16767621 -13.927516     8.071258     9.075671    -4.9253216\n",
      "   9.05101      5.919423     5.892477     1.0728025 ]\n"
     ]
    }
   ],
   "source": [
    "resNet_optimizer = optim.SGD(resNet.parameters(), lr=0.001, momentum=0.9)\n",
    "resNet_acda_optimizer = optim.SGD(resNet_acda.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# ResNet\n",
    "epochs = 2\n",
    "running_loss = train(resNet, resNet_optimizer, trainloader, epochs)\n",
    "# test_results = test(resNet, testloader)\n",
    "\n",
    "# ResNet + ACDA\n",
    "# running_loss_acda = train(resNet_acda, resNet_acda_optimizer, trainloader, epochs)\n",
    "# test_results_acda = test(resNet_acda, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "plt.plot(running_loss)\n",
    "plt.show()\n",
    "print(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameters in the models\n",
    "print(resNet.parameters(), resNet_acda.parameters())\n",
    "print(sum(p.numel() for p in resNet.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in resNet_acda.parameters() if p.requires_grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
