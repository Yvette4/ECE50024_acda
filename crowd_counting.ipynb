{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "sys.path.append(\"./libs/ACDA\")\n",
    "import Conv_DCFD as acda\n",
    "from crowd import Crowd_sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowd counting\n",
    "The paper uses ACDA trained with ImageNet as feature extractor and replace final linear layer with few standard transposed convolutional layers for recovering \n",
    "\n",
    "Note: wanted to use ShanghaiTech subset A dataset since it is smaller than UCF-QNRF.\n",
    "Download link:\n",
    "https://www.kaggle.com/datasets/tthien/shanghaitech\n",
    "\n",
    "Paper that provided download link and the processing of the ShanghaiTech dataset (need to add citation to paper!)\n",
    "https://github.com/cvlab-stonybrook/DM-Count\n",
    "\n",
    "For the crowd counting need to implement Resnet-18 (Ad-ResNet-s listed in Table A of appendix) but with out the first \"max pool\" layer and final \"average pool\" layer.\n",
    "The Ad-ResNet-s network was very similar to ResNet-18, so the code for a ResNet-18 was used as a starting point and updated with the changes described in Appendix A.4 for crodwd counting.\n",
    "Reference for how to implement ResNet-18:\n",
    "https://www.kaggle.com/code/ivankunyankin/resnet18-from-scratch-using-pytorch/notebook\n",
    "\n",
    "ResNet original paper: https://arxiv.org/pdf/1512.03385.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base code for ResNet18 came from link below, modified based on Appendix A structure\n",
    "# https://www.kaggle.com/code/ivankunyankin/resnet18-from-scratch-using-pytorch?scriptVersionId=54460301&cellId=23\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, kernel_size_acda = None, identity_downsample=None, stride=1, acda_padding=3):\n",
    "        super(Block, self).__init__()\n",
    "        self.kernel_size_acda = kernel_size_acda\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 3\n",
    "        if kernel_size_acda:\n",
    "            self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "            self.acda_conv = acda.Conv_DCFD(in_channels, in_channels, kernel_size=kernel_size_acda, stride=1, padding=acda_padding)\n",
    "            self.acda_bn = nn.BatchNorm2d(in_channels)\n",
    "            self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.kernel_size_acda:\n",
    "            x = self.acda_conv(x)\n",
    "            x = self.acda_bn(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet_18(nn.Module):\n",
    "    def __init__(self, image_channels):\n",
    "        super(ResNet_18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
    "\n",
    "    def __make_layer(self, in_channels, out_channels, stride, temp_padding=1):\n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels, temp_padding)\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x \n",
    "\n",
    "    def identity_downsample(self, in_channels, out_channels, temp_padding):   \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=temp_padding), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "class ResNet_18_ACDA(nn.Module):\n",
    "    def __init__(self, image_channels):\n",
    "        super(ResNet_18_ACDA, self).__init__()    \n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2, kernel_size=1, kernel_size_acda=7, temp_padding=4)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2, kernel_size=1, kernel_size_acda=5, acda_padding=2, temp_padding=4)\n",
    "        \n",
    "    def __make_layer(self, in_channels, out_channels, stride, kernel_size=None, kernel_size_acda=None, temp_padding=1, acda_padding=3):\n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels, temp_padding)\n",
    "\n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, \n",
    "                  stride=stride, kernel_size=kernel_size, kernel_size_acda=kernel_size_acda, acda_padding=acda_padding), \n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x \n",
    "    \n",
    "    def identity_downsample(self, in_channels, out_channels, temp_padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=temp_padding), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "resNet = ResNet_18(3)\n",
    "resNet_acda = ResNet_18_ACDA(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def train(model, optimizer, train_data, num_epochs=2):\n",
    "    mae_vals = []\n",
    "    mse_vals = []\n",
    "    loss_vals = []\n",
    "    for _ in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        mae = 0\n",
    "        mse = 0\n",
    "        count = 0\n",
    "        for i, (inputs, points, gt_discrete) in enumerate(train_data):\n",
    "            # get the inputs\n",
    "            gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "            N = inputs.size(0)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs.shape, outputs)\n",
    "            loss = mse_loss(outputs.sum(1).sum(1).sum(1), torch.from_numpy(gd_count).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "            pred_err = pred_count - gd_count\n",
    "\n",
    "            count += N\n",
    "            mae += np.mean(abs(pred_err)) * N\n",
    "            mse += np.mean(pred_err * pred_err) * N\n",
    "\n",
    "            # print statistics\n",
    "            loss_vals.append(loss.item() / N)\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 2000 mini-batches\n",
    "                # print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                loss_vals.append(running_loss / 10)\n",
    "                running_loss = 0.0\n",
    "        mae_vals.append(mae * 1.0 / count)\n",
    "        mse_vals.append(mse * 1.0 / count)\n",
    "    return loss_vals, mae_vals, mse_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of img: 300\n",
      "number of img: 182\n"
     ]
    }
   ],
   "source": [
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    gt_discretes = torch.stack(transposed_batch[2], 0)\n",
    "    return images, points, gt_discretes\n",
    "\n",
    "# load the data\n",
    "train_path = './data/ShanghaiTech/part_A/train_data/'\n",
    "test_path = './data/ShanghaiTech/part_A/test_data/'\n",
    "\n",
    "train_crowd = Crowd_sh(train_path, crop_size=112, method='train') # YVETTE - should that be 512 instead...?\n",
    "test_crowd = Crowd_sh(test_path, crop_size=112, method='val')\n",
    "\n",
    "# batch_size = 10\n",
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(train_crowd, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0,\n",
    "                                          collate_fn=train_collate,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_crowd, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0,\n",
    "                                         collate_fn=default_collate,\n",
    "                                         pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resNet_optimizer = optim.SGD(resNet.parameters(), lr=0.001, momentum=0.9)\n",
    "resNet_acda_optimizer = optim.SGD(resNet_acda.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# ResNet\n",
    "epochs = 4\n",
    "loss, mae, mse = train(resNet, resNet_optimizer, trainloader, epochs)\n",
    "\n",
    "# ResNet + ACDA\n",
    "loss_acda, mae_acda, mse_acda = train(resNet_acda, resNet_acda_optimizer, trainloader, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-18\n",
      "Parameters = 12557568, MAE = 16.226666673024496, MSE = 875.3816657511393\n",
      "\n",
      "Ad-ResNet-s\n",
      "Parameters = 9196956, MAE = 16.384166707992556, MSE = 934.4541670735678\n"
     ]
    }
   ],
   "source": [
    "resNet_params = sum(p.numel() for p in resNet.parameters() if p.requires_grad)\n",
    "resNet_acda_params = sum(p.numel() for p in resNet_acda.parameters() if p.requires_grad)\n",
    "\n",
    "print('ResNet-18')\n",
    "print(f'Parameters = {resNet_params}, MAE = {sum(mae)/len(mae)}, MSE = {sum(mse)/len(mse)}\\n')\n",
    "\n",
    "print('Ad-ResNet-s')\n",
    "print(f'Parameters = {resNet_acda_params}, MAE = {sum(mae_acda)/len(mae_acda)}, MSE = {sum(mse_acda)/len(mse_acda)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
